{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wake_words = ['tell',\n",
    "              'notify',\n",
    "              'ask',\n",
    "              'inform',\n",
    "              'message',\n",
    "              'text',\n",
    "              'reply to',\n",
    "              'mention to',\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"./cornell_movie-dialogs_corpus/movie_lines.txt\", \"rb\") \n",
    "#file = open(\"./sample.txt\", \"rb\")\n",
    "contacts, messages = [], []\n",
    "for line in file:\n",
    "    line = line.decode('utf-8', errors='ignore').strip('\\n')\n",
    "    split_line = line.split(\"+++$+++\")\n",
    "    recipent = split_line[3].strip()\n",
    "    sentences = nltk.sent_tokenize(split_line[-1].strip())\n",
    "    #print(split_line)\n",
    "    if len(recipent) < 1:\n",
    "        continue\n",
    "    else:\n",
    "        contacts.append(recipent[0]+recipent[1:].lower())\n",
    "    messages.extend(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contacts = list(set(contacts))\n",
    "random.shuffle(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_contacts = []\n",
    "for contact in contacts:\n",
    "    if '\\t\\t\\t' in contact:\n",
    "        continue\n",
    "    if ']' in contact:\n",
    "        continue\n",
    "    new_contacts.append(' '.join(contact.split()))\n",
    "contacts = new_contacts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_messages = len(messages)\n",
    "i = 0\n",
    "dataset = []\n",
    "recipents = []\n",
    "\n",
    "while i < num_messages:\n",
    "    # Wake words\n",
    "    wake_word = random.sample(wake_words, 1)[0]\n",
    "    if random.random() < 0.5:\n",
    "        command = \"Can you \"\\\n",
    "                  + wake_word\\\n",
    "                  + \" \"              \n",
    "    else:\n",
    "        command = wake_word[0].upper()\\\n",
    "                  + wake_word[1:]\\\n",
    "                  + \" \"\n",
    "            \n",
    "    # Recipent\n",
    "    contact = random.sample(contacts, 1)[0]\n",
    "    command += contact\n",
    "    recipents.append(contact)\n",
    "    \n",
    "    # Message(s)\n",
    "    if random.random() < 0.5:\n",
    "        command += ' that'\n",
    "        \n",
    "    command += \" \" + messages[i]\n",
    "    i += 1\n",
    "    \n",
    "    if random.random() < 0.25 and i < num_messages:\n",
    "        command += \" and that \"\\\n",
    "                   + messages[i]\n",
    "        i += 1\n",
    "        \n",
    "    dataset.append(command)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contacts_dict = defaultdict(list)\n",
    "\n",
    "for contact in contacts:\n",
    "    contact_split = contact.split()\n",
    "    contacts_dict[contact_split[0].lower()].append(contact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, recipents, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(dataset, open('command_dataset.pickle', 'wb'))\n",
    "pickle.dump(recipents, open('recipents_dataset.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_recipent_rule_based(command):\n",
    "    \n",
    "    wake_word_said = False\n",
    "    words = command.split()\n",
    "    i = 0\n",
    "    \n",
    "    # only searching for recipent if wake word is said\n",
    "    while i < len(words):\n",
    "        if words[i].lower() in wake_words:\n",
    "            i += 1\n",
    "            wake_word_said = True\n",
    "            break\n",
    "        elif ' '.join(words[i:i+2]).lower() in wake_words:\n",
    "            i += 2\n",
    "            wake_word_said = True\n",
    "            break\n",
    "        i += 1\n",
    "        \n",
    "    # rearching for first word of recipent name\n",
    "    if wake_word_said:\n",
    "        while words[i].lower() not in contacts_dict and i < len(words):\n",
    "            i += 1\n",
    "            \n",
    "    # when found, find the whole recipent name\n",
    "    if i < len(words):\n",
    "        if words[i].lower() in contacts_dict:\n",
    "            sub_sentence = ' '.join(words[i:])\n",
    "            max_matching_len = 0\n",
    "\n",
    "            for name in contacts_dict[words[i].lower()]:\n",
    "                if name in sub_sentence and len(name)>max_matching_len:\n",
    "                    max_matching_len = len(name)\n",
    "                    recipent = name\n",
    "            try:\n",
    "                return recipent\n",
    "            except:\n",
    "                print(contacts_dict[words[i].lower()])\n",
    "\n",
    "        i += 1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rb = []\n",
    "for test in X_test:\n",
    "    y_pred_rb.append(find_recipent_rule_based(test))\n",
    "\n",
    "correct = 0\n",
    "for y, y_bar in zip(y_test, y_pred_rb):\n",
    "    correct += int(y==y_bar)\n",
    "    if y != y_bar:\n",
    "        print(y, y_bar)\n",
    "\n",
    "correct/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feature_label(command, label, n):\n",
    "\n",
    "    words = nltk.word_tokenize(command)\n",
    "    words=[word.lower() for word in words if word.isalpha()]\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    num_words = len(words)\n",
    "    for i, word in enumerate(words):\n",
    "        feature = []\n",
    "        for j in range(n, 0, -1):\n",
    "            k = i-j\n",
    "            if k >= 0:\n",
    "                feature.append(words[k])\n",
    "            else:\n",
    "                feature.append('NA')\n",
    "        for j in range(1, n+1):\n",
    "            k = i+j\n",
    "            if k < num_words:\n",
    "                feature.append(words[k])\n",
    "            else:\n",
    "                feature.append('NA')\n",
    "        feature.append(i)\n",
    "        features.append(feature)\n",
    "\n",
    "        if word in label.lower().split():\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return features, labels\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_neighbor = 1\n",
    "features, labels = extract_feature_label(X_train[2], y_train[2], n_neighbor)\n",
    "\n",
    "X_train_word_token, y_train_word_token = [], []\n",
    "for command, recipent in zip(X_train, y_train):\n",
    "    features, labels = extract_feature_label(command, recipent,1)\n",
    "    X_train_word_token += features\n",
    "    y_train_word_token += labels\n",
    "\n",
    "ratio = sum(y_train_word_token)/len(y_train_word_token)\n",
    "\n",
    "train_word_token_blc = []\n",
    "for feature, label in zip(X_train_word_token, y_train_word_token):\n",
    "    if label == 0 and random.random() > ratio:\n",
    "        continue\n",
    "    train_word_token_blc.append(feature+[label])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fwd, back = [], []\n",
    "for i in range(n_neighbor):\n",
    "    fwd.append('-'+str(i+1)+'_loc')\n",
    "    back.append('+'+str(i+1)+'_loc')\n",
    "fwd.reverse()\n",
    "col_names = fwd+back+['loc','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_word_token_blc, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1_loc</th>\n",
       "      <th>+1_loc</th>\n",
       "      <th>loc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>that</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know</td>\n",
       "      <td>that</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>notify</td>\n",
       "      <td>all</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all</td>\n",
       "      <td>chicks</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ai</td>\n",
       "      <td>no</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   -1_loc  +1_loc  loc  label\n",
       "0    text    that    3      1\n",
       "1    know    that   10      0\n",
       "2  notify     all    1      1\n",
       "3     all  chicks    3      0\n",
       "4      ai      no   16      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "stop_words = ENGLISH_STOP_WORDS.union('NA')\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "    \n",
    "\n",
    "CV = CountVectorizer(stop_words=stop_words, tokenizer=LemmaTokenizer())\n",
    "X_fwd = CV.fit_transform(train_df['-1_loc'])\n",
    "X_bck = CV.fit_transform(train_df['+1_loc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "foo = np.concatenate((X_fwd.toarray(), X_bck.toarray()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fwd.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(745517, 17398)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bck.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
